{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear output esc r y\n",
    "# https://scikit-learn.org/stable/modules/unsupervised_reduction.html\n",
    "# https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "#import csv\n",
    "#with open('X_train.csv', newline='') as csvfile:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "xdata = pd.read_csv ('X_train.csv', nrows=5)\n",
    "ydata = pd.read_csv('y_train.csv',nrows=5)\n",
    "\n",
    "merged = pd.concat([xdata, ydata['y']], axis=1, sort=False)\n",
    "print(xdata.drop(['id'],axis=1))\n",
    "#print(ydata['y'])\n",
    "#print(np.transpose(ydata.values)[1:][0])\n",
    "#print(len(np.transpose(xdata.values)[1:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We started with 832 features but retained only 416 of them!\n"
     ]
    }
   ],
   "source": [
    "#GenericUnivariateSelect\n",
    "# https://www.kaggle.com/residentmario/automated-feature-selection-with-sklearn\n",
    "\n",
    "from sklearn.feature_selection import GenericUnivariateSelect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "  \n",
    "xdata = pd.read_csv('x_train_mean.csv')\n",
    "ydata = pd.read_csv('y_train.csv')\n",
    "\n",
    "X=xdata.drop(['id'],axis=1)\n",
    "y=ydata['y']\n",
    "\n",
    "trans = GenericUnivariateSelect(score_func=lambda X, y: X.mean(axis=0), mode='percentile', param=50)\n",
    "chars_X_trans = trans.fit_transform(X, y)\n",
    "\n",
    "print(\"We started with {0} features but retained only {1} of them!\".format(X.shape[1], chars_X_trans.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We started with 831 features but retained only 416 of them!\n"
     ]
    }
   ],
   "source": [
    "#pt 2\n",
    "from sklearn.feature_selection import GenericUnivariateSelect\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "  \n",
    "xdata = pd.read_csv('x_train_mean.csv',nrows=20)\n",
    "ydata = pd.read_csv('y_train.csv',nrows=20)\n",
    "\n",
    "X=xdata.drop(['id'],axis=1)\n",
    "y=ydata['y']\n",
    "\n",
    "#kepler_mutual_information = mutual_info_classif(X, y)\n",
    "\n",
    "trans = GenericUnivariateSelect(score_func=mutual_info_classif, mode='percentile', param=50)\n",
    "kepler_X_trans = trans.fit_transform(X, y)\n",
    "\n",
    "print(\"We started with {0} features but retained only {1} of them!\".format(X.shape[1] - 1, kepler_X_trans.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Regression\n",
    "# http://www.webgraphviz.com then copy .dot file\n",
    "\n",
    "# import numpy package for arrays and stuff \n",
    "import numpy as np  \n",
    "  \n",
    "# import matplotlib.pyplot for plotting our result \n",
    "import matplotlib.pyplot as plt \n",
    "  \n",
    "# import pandas for importing csv files  \n",
    "import pandas as pd  \n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "  \n",
    "xdata = pd.read_csv('x_train_mean.csv')\n",
    "ydata = pd.read_csv('y_train.csv')\n",
    "\n",
    "X=xdata.drop(['id'],axis=1)\n",
    "y=ydata['y']\n",
    "    \n",
    "# create a regressor object \n",
    "regressor = DecisionTreeRegressor(random_state = 0)  \n",
    "  \n",
    "# fit the regressor with X and Y data \n",
    "regressor.fit(X, y) \n",
    "\n",
    "\n",
    "# import export_graphviz \n",
    "from sklearn.tree import export_graphviz  \n",
    "  \n",
    "# export the decision tree to a tree.dot file \n",
    "# for visualizing the plot easily anywhere \n",
    "export_graphviz(regressor, out_file ='tree.dot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value in each column : \n",
      "6.503016520821267e+18\n",
      "Maximum value in each column : \n",
      "-3.76020076348273e+22\n",
      "False\n",
      "True\n",
      "Best alpha using built-in LassoCV: 31006708878259097960448.000000\n",
      "Best score using built-in LassoCV: 0.000000\n"
     ]
    }
   ],
   "source": [
    "#Feature selection using embedded method\n",
    "import pandas as pd\n",
    "import scipy.stats as sst\n",
    "#importing libraries\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "\n",
    "xdata = pd.read_csv('x_train_mean.csv',nrows=10)\n",
    "ydata = pd.read_csv('y_train.csv',nrows=10)\n",
    "\n",
    "X=xdata.drop(['id'],axis=1)\n",
    "y=ydata['y']\n",
    "\n",
    "reg = LassoCV()\n",
    "reg.fit(X, y)\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
    "print(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\n",
    "coef = pd.Series(reg.coef_, index = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x183      x537\n",
      "x183  1.000000 -0.083316\n",
      "x537 -0.083316  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Feature selection using Pearson Correlation\n",
    "#https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b\n",
    "import pandas as pd\n",
    "import scipy.stats as sst\n",
    "#importing libraries\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "\n",
    "xdata = pd.read_csv('X_train.csv')\n",
    "ydata = pd.read_csv('y_train.csv')\n",
    "\n",
    "merged = pd.concat([xdata, ydata['y']], axis=1, sort=False)\n",
    "\n",
    "#df1 = merged.iloc[:, 0:]\n",
    "#plt.figure(figsize=(len(merged),len(merged)))\n",
    "cor = merged.corr()\n",
    "#sns.heatmap(cor,annot=True,cmap=plt.cm.Reds)\n",
    "\n",
    "#Correlation with output variable\n",
    "cor_target = abs(cor[\"y\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.5]\n",
    "relevant_features\n",
    "\n",
    "print(merged[['x183','x537']].corr()) # not correlated so we don't drop either\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection using Chi2 test\n",
    "#Doesn't work since its a cmbi of features\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.stats as sst\n",
    "\n",
    "xdata = pd.read_csv('X_train.csv')\n",
    "ydata = pd.read_csv('y_train.csv')\n",
    "\n",
    "xdatabycol = np.transpose(xdata.values)[1:] #line 0 -> x0 etc.\n",
    "ydatabycol = np.transpose(ydata.values)[1:][0]\n",
    "\n",
    "for attr_index in range(len(xdatabycol)):\n",
    "    #attr_sol = np.column_stack(xdatabycol[attr_index],ydatabycol)\n",
    "    crosstab = pd.crosstab(xdatabycol[attr_index],ydatabycol)\n",
    "    chi2,p,_,_ = sst.chi2_contingency(crosstab)\n",
    "    if (p > 0.05):\n",
    "        print(attr_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "df = pd.read_csv('X_train.csv',nrows=5)\n",
    "\n",
    ">>> \n",
    ">>> X, y = load_iris(return_X_y=True)\n",
    ">>> X.shape\n",
    "(150, 4)\n",
    ">>> clf = ExtraTreesClassifier(n_estimators=50)\n",
    ">>> clf = clf.fit(X, y)\n",
    ">>> clf.feature_importances_  \n",
    "array([ 0.04...,  0.05...,  0.4...,  0.4...])\n",
    ">>> model = SelectFromModel(clf, prefit=True)\n",
    ">>> X_new = model.transform(X)\n",
    ">>> X_new.shape               \n",
    "(150, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.13.1. Removing features with low variance\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv ('x_train_KNN.csv',nrows=10)\n",
    "\n",
    "#p = 0.80\n",
    "#sel = VarianceThreshold(threshold=(p * (1 - p)))\n",
    "\n",
    "mapper = DataFrameMapper([(df.columns, StandardScaler())])\n",
    "scaled_features = mapper.fit_transform(df.copy())\n",
    "scaled_features_df = pd.DataFrame(scaled_features, index=df.index, columns=df.columns)\n",
    "\n",
    "#print(df_fitted)\n",
    "\n",
    "print(scaled_features_df == df)\n",
    "print(scaled_features_df)\n",
    "    \n",
    "#for index, row in df.iterrows():\n",
    "#    print(row)\n",
    "#    for attr in row:\n",
    "#        print(row(attr))\n",
    "    \n",
    "#    print(\"---------------\")\n",
    "   \n",
    "\n",
    "\n",
    "# for attr in range(df.shape[1]):\n",
    "#        print(row)\n",
    "#        print(attr)\n",
    "#        if (dfcopy[row][attr] != df_fitted[row][attr]):\n",
    "#            print(i)\n",
    "#            print(dfcopy[row][attr])\n",
    "#            print(df_fitted[row][attr])\n",
    "\n",
    "np.array_equal(scaled_features_df,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##unsupervised dim reduc https://scikit-learn.org/stable/modules/unsupervised_reduction.html\n",
    "\n",
    "## FEATURE AGGLOMERATION\n",
    "\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df=pd.read_csv ('x_train_KNN.csv',nrows=10)\n",
    "labels = pd.read_csv('Y_train.csv',nrows=10)\n",
    "\n",
    "#set n_clusters to 2, the output will be two columns of agglomerated features ( iris has 4 features)\n",
    "agglo=FeatureAgglomeration(n_clusters=2).fit_transform(labels)\n",
    "\n",
    "print(agglo)\n",
    "print(agglo[:,0])\n",
    "\n",
    "#plotting\n",
    "#color=[]\n",
    "#for i in labels:\n",
    "#    if i=='Iris-setosa':\n",
    "#        color.append('g')\n",
    "#    if  i=='Iris-versicolor':\n",
    "#        color.append('b')\n",
    "#    if i=='Iris-virginica':\n",
    "#        color.append('r')\n",
    "#plt.scatter(agglo[:,0],agglo[:,1],c=color)\n",
    "#plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
