{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Import data and format data, initialize variables\n",
    "X = pd.read_csv('x_train__wo_outlier_KNN.csv',delimiter=',', index_col='id')\n",
    "ydata = pd.read_csv('y_train_wo_outlier.csv',delimiter=',', index_col='id')\n",
    "\n",
    "X_test = pd.read_csv('x_test_KNN.csv',delimiter=',', index_col='id')\n",
    "\n",
    "y=ydata['y']\n",
    "\n",
    "num_features=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pearson Correlation\n",
    "def Cor_selector(X, y,num_features):\n",
    "    print(\"Cor\")\n",
    "    cor_list = []\n",
    "    feature_name = X.columns.tolist()\n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in X.columns.tolist():\n",
    "        cor = np.corrcoef(X[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    # replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    # feature name\n",
    "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_features:]].columns.tolist()\n",
    "    # feature selection? 0 for not select, 1 for select\n",
    "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "    return cor_support, cor_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi squared test\n",
    "def Chi_selector(X,y,num_features):\n",
    "    print(\"Chi\")\n",
    "    #Normalize values as required for Chi2 test\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    chi_selector = SelectKBest(chi2, k=num_features)\n",
    "    chi_selector.fit(X_norm, y)\n",
    "    chi_support = chi_selector.get_support()\n",
    "    chi_feature = X.loc[:,chi_support].columns.tolist()\n",
    "    return chi_support, chi_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recursive Feature Elimination\n",
    "def RFE_selector(X,y,num_features):\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    #max_iter bumped up to 2000 in order to converge (1000 default too small)\n",
    "    rfe_selector = RFE(estimator=LogisticRegression(max_iter=2000), n_features_to_select=num_features, step=10, verbose=5)\n",
    "    rfe_selector.fit(X_norm, y)\n",
    "    rfe_support = rfe_selector.get_support()\n",
    "    rfe_feature = X.loc[:,rfe_support].columns.tolist()\n",
    "    return rfe_support, rfe_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso : SelectFromModel (L1 norm)\n",
    "def LassoModel(X,y,num_features):\n",
    "    print(\"Lasso\")\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l1\",solver='liblinear'), max_features=num_features)\n",
    "    embeded_lr_selector.fit(X_norm, y)\n",
    "    embeded_lr_support = embeded_lr_selector.get_support()\n",
    "    embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\n",
    "    return embeded_lr_support, embeded_lr_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tree-based : SelectFromModel\n",
    "def TreebasedModel(X,y,num_features):\n",
    "    print(\"TBM\")\n",
    "    embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_features)\n",
    "    embeded_rf_selector.fit(X, y)\n",
    "    embeded_rf_support = embeded_rf_selector.get_support()\n",
    "    embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\n",
    "    return embeded_rf_support, embeded_rf_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LightGBM\n",
    "def LightGBM(X,y,num_features):\n",
    "    print(\"GBM\")\n",
    "    lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n",
    "            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "    embeded_lgb_selector = SelectFromModel(lgbc, max_features=num_features)\n",
    "    embeded_lgb_selector.fit(X, y)\n",
    "    embeded_lgb_support = embeded_lgb_selector.get_support()\n",
    "    embeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()\n",
    "    return embeded_lgb_support, embeded_lgb_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cor\n",
      "Chi\n",
      "Fitting estimator with 832 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 822 features.\n",
      "Fitting estimator with 812 features.\n",
      "Fitting estimator with 802 features.\n",
      "Fitting estimator with 792 features.\n",
      "Fitting estimator with 782 features.\n",
      "Fitting estimator with 772 features.\n",
      "Fitting estimator with 762 features.\n",
      "Fitting estimator with 752 features.\n",
      "Fitting estimator with 742 features.\n",
      "Fitting estimator with 732 features.\n",
      "Fitting estimator with 722 features.\n",
      "Fitting estimator with 712 features.\n",
      "Fitting estimator with 702 features.\n",
      "Fitting estimator with 692 features.\n",
      "Fitting estimator with 682 features.\n",
      "Fitting estimator with 672 features.\n",
      "Fitting estimator with 662 features.\n",
      "Fitting estimator with 652 features.\n",
      "Fitting estimator with 642 features.\n",
      "Fitting estimator with 632 features.\n",
      "Fitting estimator with 622 features.\n",
      "Fitting estimator with 612 features.\n",
      "Fitting estimator with 602 features.\n",
      "Fitting estimator with 592 features.\n",
      "Fitting estimator with 582 features.\n",
      "Fitting estimator with 572 features.\n",
      "Fitting estimator with 562 features.\n",
      "Fitting estimator with 552 features.\n",
      "Fitting estimator with 542 features.\n",
      "Fitting estimator with 532 features.\n",
      "Fitting estimator with 522 features.\n",
      "Fitting estimator with 512 features.\n",
      "Fitting estimator with 502 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 392 features.\n",
      "Fitting estimator with 382 features.\n",
      "Fitting estimator with 372 features.\n",
      "Fitting estimator with 362 features.\n",
      "Fitting estimator with 352 features.\n",
      "Fitting estimator with 342 features.\n",
      "Fitting estimator with 332 features.\n",
      "Fitting estimator with 322 features.\n",
      "Fitting estimator with 312 features.\n",
      "Fitting estimator with 302 features.\n",
      "Fitting estimator with 292 features.\n",
      "Fitting estimator with 282 features.\n",
      "Fitting estimator with 272 features.\n",
      "Fitting estimator with 262 features.\n",
      "Fitting estimator with 252 features.\n",
      "Fitting estimator with 242 features.\n",
      "Fitting estimator with 232 features.\n",
      "Fitting estimator with 222 features.\n",
      "Fitting estimator with 212 features.\n",
      "Fitting estimator with 202 features.\n"
     ]
    }
   ],
   "source": [
    "#MAIN\n",
    "pd.set_option('display.max_rows', 250)\n",
    "feature_name = X.columns.tolist()\n",
    "\n",
    "#Initialize variables corresponding to respective tests\n",
    "cor_support,_ = Cor_selector(X, y,num_features)\n",
    "chi_support,_ = Chi_selector(X,y,num_features)\n",
    "rfe_support,_ = RFE_selector(X,y,num_features)\n",
    "embeded_lr_support,_ = LassoModel(X,y,num_features)\n",
    "embeded_rf_support,_ = TreebasedModel(X,y,num_features)\n",
    "embeded_lgb_support,_ = LightGBM(X,y,num_features)\n",
    "\n",
    "# put all selection together\n",
    "feature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,\n",
    "                                    'Random Forest':embeded_rf_support, 'LightGBM':embeded_lgb_support})\n",
    "# count the selected times for each feature\n",
    "feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
    "# display the top 100\n",
    "feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
    "feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "feature_selection_df.head(num_features)\n",
    "#Count the selected times for each feature and sorts them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the csv file with removed features for train data\n",
    "    \n",
    "top_features = feature_selection_df.copy()\n",
    "filtered_X = X.copy()\n",
    "\n",
    "last_features = top_features.tail(len(top_features)-num_features)\n",
    "\n",
    "for index, row in last_features.iterrows():\n",
    "    filtered_X = filtered_X.drop(columns=[row.Feature])\n",
    "\n",
    "filtered_X.to_csv('x_train_wo_outliers_dataval_200f.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the csv file with removed features for test data\n",
    "    \n",
    "top_features = feature_selection_df.copy()\n",
    "filtered_X_test = X_test.copy()\n",
    "\n",
    "last_features = top_features.tail(len(top_features)-num_features)\n",
    "\n",
    "for index, row in last_features.iterrows():\n",
    "    filtered_X_test = filtered_X_test.drop(columns=[row.Feature])\n",
    "\n",
    "filtered_X_test.to_csv('x_test_dataval_200f.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
