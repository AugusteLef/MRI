{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.model_selection import train_test_split as split_TT\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, SelectKBest\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_origin =  pd.read_csv(\"x_train.csv\")\n",
    "y_train_origin = pd.read_csv(\"y_train.csv\")\n",
    "\n",
    "x_test =  pd.read_csv(\"x_test.csv\", delimiter=\",\", index_col='id')\n",
    "\n",
    "#formatting\n",
    "train_data = pd.merge(left=x_train_origin, right=y_train_origin, how='inner').drop(columns=['id'])\n",
    "x_train = x_train_origin.drop(columns=['id'])\n",
    "y = y_train_origin['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.fillna(x_train.median())\n",
    "x_test = x_test.fillna(x_test.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier drop\n",
    "\n",
    "###### We use Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num lines before drop :  (1191, 832)\n",
      "Num lines after drop :  (1191, 832)\n"
     ]
    }
   ],
   "source": [
    "clf = IsolationForest(contamination='auto', random_state=323)\n",
    "clf.fit(x_train)\n",
    "yesOrNoOutlier = clf.predict(x_train.values)\n",
    "\n",
    "print(\"Num lines before drop : \", x_train.shape)\n",
    "\n",
    "for num,line in enumerate(yesOrNoOutlier):\n",
    "    if not line == 1:\n",
    "        x_train = x_train.drop([num])\n",
    "\n",
    "print(\"Num lines after drop : \", x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We define all feature selection functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation\n",
    "def CorrSelector(xy_data,threshold):\n",
    "    corr = xy_data.corr()\n",
    "    top_features = corr.index[abs(corr['y']>threshold)]\n",
    "    \n",
    "    ##Displays best features correlation as a table\n",
    "    #plt.subplots(figsize=(12, 8))\n",
    "    top_corr = xy_data[top_features].corr()\n",
    "    sns.heatmap(top_corr, annot=True)\n",
    "    plt.show()\n",
    "    \n",
    "    #Displays best features as graphs\n",
    "    col = corr.index[abs(corr['y']>0.38)]\n",
    "    sns.set(style='ticks')\n",
    "    sns.pairplot(xy_data[col],kind='reg')\n",
    "    \n",
    "    return top_features, top_corr\n",
    "\n",
    "\n",
    "#Remove Low Variance\n",
    "def HighVarSelector(X_train, X_test):\n",
    "    threshold = VarianceThreshold(threshold=(.90 * (1 - .90)))\n",
    "    \n",
    "    filtered_X_train = threshold.fit_transform(X_train)\n",
    "    filtered_X_test = threshold.transform(X_test)\n",
    "    return pd.DataFrame(filtered_X_train), pd.DataFrame(filtered_X_test)\n",
    "\n",
    "\n",
    "#KBest Selector with mutual information regression (removes similar data)\n",
    "def MutualInfoSelector(X_train,X_test,y,num_features):\n",
    "    #TODO remove random_state\n",
    "    k_best = SelectKBest(score_func=lambda X,y:mutual_info_regression(X, y, random_state=50), k=num_features)\n",
    "    X_train_kbest = k_best.fit_transform(X_train,y)\n",
    "    X_test_kbest = k_best.transform(X_test)\n",
    "    return pd.DataFrame(X_train_kbest),pd.DataFrame(X_test_kbest)\n",
    "\n",
    "\n",
    "#GradientBoost\n",
    "def GBoostSelector(X_train,X_test,y):\n",
    "    #TODO check params, remove random_state\n",
    "    GBoost = GradientBoostingRegressor(max_depth=20, n_estimators=300, min_samples_leaf=15, min_samples_split=10, random_state =50)\n",
    "    GBoost.fit(X_train.values, y.values)\n",
    "    \n",
    "    #Top features selection, sorts them by importance\n",
    "    top_df = pd.DataFrame(data=GBoost.feature_importances_, columns=['value'])\n",
    "    top_df['feature'] = np.asarray(top_df.index)\n",
    "    top_df = top_df.sort_values(axis=0, by='value', ascending=False)\n",
    "    #TODO vary this param in order to find optimal cut\n",
    "    top_df = top_df[top_df['value'] > 0.006]\n",
    "\n",
    "    #Displays best features and their score\n",
    "    sns.barplot(x=top_df['feature'], y=top_df['value'])\n",
    "    \n",
    "    gboost_selector = SelectFromModel(GBoost, prefit=True)\n",
    "    X_train_bestfeatures = gboost_selector.transform(X_train)\n",
    "    X_test_bestfeatures = gboost_selector.transform(X_test)\n",
    "    \n",
    "    return pd.DataFrame(X_train_bestfeatures),pd.DataFrame(X_test_bestfeatures)\n",
    "\n",
    "\n",
    "#Chi squared test\n",
    "def Chi_selector(X,y,num_features):\n",
    "    print(\"Chi\")\n",
    "    #Normalize values as required for Chi2 test\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    chi_selector = SelectKBest(chi2, k=num_features)\n",
    "    chi_selector.fit(X_norm, y)\n",
    "    chi_support = chi_selector.get_support()\n",
    "    chi_feature = X.loc[:,chi_support].columns.tolist()\n",
    "    return chi_support, chi_feature\n",
    "\n",
    "\n",
    "#Recursive Feature Elimination\n",
    "def RFE_selector(X_train,X_test,y,num_features):\n",
    "    estimator = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, min_samples_leaf=15, min_samples_split=10,random_state=50)\n",
    "    rfe_selector = RFE(estimator=estimator, n_features_to_select=num_features, step=0.1, verbose=1)\n",
    "    x_train_rfe = rfe_selector.fit_transform(X_train, y)\n",
    "    x_test_rfe = rfe_selector.transform(X_test)\n",
    "    rfe_support = rfe_selector.get_support()\n",
    "    rfe_feature = x_train.loc[:,rfe_support].columns.tolist()\n",
    "    return rfe_support, rfe_feature, pd.DataFrame(x_train_rfe),pd.DataFrame(x_test_rfe)\n",
    "\n",
    "\n",
    "#Lasso : SelectFromModel (L1 norm)\n",
    "def LassoModel(X,y,num_features):\n",
    "    print(\"Lasso\")\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l1\",solver='liblinear'), max_features=num_features)\n",
    "    embeded_lr_selector.fit(X_norm, y)\n",
    "    embeded_lr_support = embeded_lr_selector.get_support()\n",
    "    embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\n",
    "    return embeded_lr_support, embeded_lr_feature\n",
    "\n",
    "\n",
    "#Tree-based : SelectFromModel\n",
    "def TreebasedModel(X,y,num_features):\n",
    "    print(\"TBM\")\n",
    "    embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_features)\n",
    "    embeded_rf_selector.fit(X, y)\n",
    "    embeded_rf_support = embeded_rf_selector.get_support()\n",
    "    embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\n",
    "    return embeded_rf_support, embeded_rf_feature\n",
    "\n",
    "\n",
    "#LightGBM\n",
    "def LightGBM(X,y,num_features):\n",
    "    print(\"GBM\")\n",
    "    lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n",
    "            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "    embeded_lgb_selector = SelectFromModel(lgbc, max_features=num_features)\n",
    "    embeded_lgb_selector.fit(X, y)\n",
    "    embeded_lgb_support = embeded_lgb_selector.get_support()\n",
    "    embeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()\n",
    "    return embeded_lgb_support, embeded_lgb_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection takes place here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before Feature Selection: (1212, 832) \n",
      "Test shape before Feature Selection: (776, 832) \n",
      "Train shape after HighVarSelector: (1212, 666) \n",
      "Test shape after HighVarSelector: (776, 666) \n",
      "Train shape after MutualInfoSelector: (1212, 600) \n",
      "Test shape after MutualInfoSelector: (776, 600) \n",
      "Train shape after GBoostSelector: (1212, 600) \n",
      "Test shape after GBoostSelector: (776, 600) \n",
      "Fitting estimator with 600 features.\n",
      "Fitting estimator with 540 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 360 features.\n",
      "Fitting estimator with 300 features.\n",
      "Fitting estimator with 240 features.\n",
      "Fitting estimator with 180 features.\n",
      "Fitting estimator with 120 features.\n",
      "Train shape after RFESelector: (1212, 60) \n",
      "Test shape after RFESelector: (776, 60) \n"
     ]
    }
   ],
   "source": [
    "#Set display options\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "#Init variables (number of retained top features)\n",
    "num_features=600\n",
    "feature_name = x_train.columns.tolist()\n",
    "\n",
    "print (\"Shape before Feature Selection: {} \".format(x_train.shape))\n",
    "print (\"Test shape before Feature Selection: {} \".format(x_test.shape))\n",
    "\n",
    "### HighVar -> MutualInfo -> GBoost dev2\n",
    "### HighVar -> MutualInfo -> RFE dev3\n",
    "\n",
    "\n",
    "###Correlation not used\n",
    "#top_feat, top_corr = CorrSelector(train_data,0.3)\n",
    "#print \"Shape after CorrSelector: {} \".format(x_train.shape)\n",
    "\n",
    "###HighVar\n",
    "x_train, x_test = HighVarSelector(x_train,x_test)\n",
    "print (\"Train shape after HighVarSelector: {} \".format(x_train.shape))\n",
    "print (\"Test shape after HighVarSelector: {} \".format(x_test.shape))\n",
    "\n",
    "###Mutual Info\n",
    "x_train, x_test = MutualInfoSelector(x_train, x_test, y,num_features)\n",
    "print (\"Train shape after MutualInfoSelector: {} \".format(x_train.shape))\n",
    "print (\"Test shape after MutualInfoSelector: {} \".format(x_test.shape))\n",
    "\n",
    "###GBoost\n",
    "#x_train,x_test = GBoostSelector(x_train,x_test, y)\n",
    "print (\"Train shape after GBoostSelector: {} \".format(x_train.shape))\n",
    "print (\"Test shape after GBoostSelector: {} \".format(x_test.shape))\n",
    "\n",
    "#RFESelector\n",
    "rfe_support,_,x_train,x_test = RFE_selector(x_train,x_test,y,60)\n",
    "print (\"Train shape after RFESelector: {} \".format(x_train.shape))\n",
    "print (\"Test shape after RFESelector: {} \".format(x_test.shape))\n",
    "\n",
    "\n",
    "\n",
    "x_train.to_csv('x_train_dev3.csv', index=False)\n",
    "x_test.to_csv('x_test_dev3.csv', index=False)\n",
    "\n",
    "\n",
    "###CHI\n",
    "\n",
    "###RFE\n",
    "\n",
    "\n",
    "###Lasso\n",
    "\n",
    "\n",
    "###TreeBased\n",
    "\n",
    "\n",
    "###LightGBM\n",
    "\n",
    "\n",
    "\n",
    "#chi_support,_ = Chi_selector(X,y,num_features)\n",
    "#rfe_support,_ = RFE_selector(X,y,num_features)\n",
    "#embeded_lr_support,_ = LassoModel(X,y,num_features)\n",
    "#embeded_rf_support,_ = TreebasedModel(X,y,num_features)\n",
    "#embeded_lgb_support,_ = LightGBM(X,y,num_features)\n",
    "\n",
    "# put all selection together\n",
    "#feature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,'Random Forest':embeded_rf_support, 'LightGBM':embeded_lgb_support})\n",
    "# count the selected times for each feature\n",
    "#feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
    "# display the top 100\n",
    "#feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
    "#feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "#feature_selection_df.head(num_features)\n",
    "#Count the selected times for each feature and sorts them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#loading different data\n",
    "x_train =  pd.read_csv(\"x_train_dev3.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")['y']\n",
    "\n",
    "x_test = pd.read_csv(\"x_test_dev3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "\n",
    "def r2_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train.values)\n",
    "    r2= cross_val_score(model, x_train.values, y_train.values, scoring=\"r2\", cv = kf, )\n",
    "    return(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=5)\n",
    "\n",
    "#When doing rendu NN we train on x_train y_train and get output from x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out multiple regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Determine best params\n",
    "#grid={\"n_estimators\":[300], \"learning_rate\":[0.1], \"max_depth\":[i for i in range(3, 15)], \"max_features\":[1.0, 0.1, 0.3]}\n",
    "#GBoost = GradientBoostingRegressor(min_samples_leaf=15, min_samples_split=10, random_state =5)\n",
    "#logreg_cv=GridSearchCV(GBoost,grid,cv=10, scoring='r2', verbose=2)\n",
    "#logreg_cv.fit(x_train.values, y_train.values)\n",
    "\n",
    "#print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "#print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params OK\n",
    "gBoost = GradientBoostingRegressor(n_estimators=300, learning_rate=0.1,\n",
    "                                   max_depth=10, max_features=0.1 ,\n",
    "                                   min_samples_leaf=15, min_samples_split=10,\n",
    "                                   random_state =50)\n",
    "\n",
    "gBoost.fit(x_train.values, y_train.values)\n",
    "y_gBoost_train = gBoost.predict(x_train.values)\n",
    "y_gBoost_test = gBoost.predict(x_test.values)  #pour le rendu on garde ca\n",
    "\n",
    "#print(r2_score(y_test, y_gBoost_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Determine best params\n",
    "#grid={\"n_estimators\":[300], \"max_depth\":[i for i in range(3, 15)], \"max_features\":[1.0, 0.1, 0.3]}\n",
    "#rf = RandomForestRegressor(random_state=5, max_features='sqrt')\n",
    "#rf_cv=GridSearchCV(rf,grid,cv=10, scoring='r2', verbose=2)\n",
    "#rf_cv.fit(x_train.values, y_train.values)\n",
    "\n",
    "#print(\"tuned hpyerparameters :(best parameters) \",rf_cv.best_params_)\n",
    "#print(\"accuracy :\",rf_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params ok \n",
    "random_forest = RandomForestRegressor(max_depth=14, max_features=0.3, n_estimators=300, random_state=50)\n",
    "random_forest.fit(x_train.values, y_train.values)\n",
    "f_i = random_forest.feature_importances_\n",
    "\n",
    "y_rf_train = random_forest.predict(x_train.values)\n",
    "y_rf_test = random_forest.predict(x_test.values)\n",
    "\n",
    "#print(r2_score(y_test, y_rf_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Determining best params\n",
    "#grid={\"max_depth\":[i for i in range(5, 10)]}\n",
    "#model_xgb = xgb.XGBRegressor(n_estimators=300, learning_rate=0.1, random_state =5, nthread = -1)\n",
    "#model_xgb_cv=GridSearchCV(model_xgb, grid, cv=10, scoring='r2', verbose=2)\n",
    "#model_xgb_cv.fit(x_train, y_train.values)\n",
    "#\n",
    "#print(\"tuned hpyerparameters :(best parameters) \",model_xgb_cv.best_params_)\n",
    "#print(\"accuracy :\",model_xgb_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params ok\n",
    "model_xgb = xgb.XGBRegressor(learning_rate=0.1, max_depth=6, \n",
    "                              n_estimators=300,\n",
    "                             random_state =50, nthread = -1)\n",
    "model_xgb.fit(x_train.values, y_train.values)\n",
    "y_xgb_train = model_xgb.predict(x_train.values)\n",
    "y_xgb_test = model_xgb.predict(x_test.values)\n",
    "\n",
    "#print(r2_score(y_test, y_xgb_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hist Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Determining best params\n",
    "#grid={\"max_depth\":[i for i in range(14, 20)]}\n",
    "#est = HistGradientBoostingRegressor(random_state=50)\n",
    "#logreg_cv=GridSearchCV(est,grid,cv=10, scoring='r2', verbose=2)\n",
    "#logreg_cv.fit(x_train.values, y_train.values)\n",
    "#\n",
    "#print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "#print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params OK\n",
    "hgBoost = HistGradientBoostingRegressor(max_depth=16, random_state=50)\n",
    "\n",
    "hgBoost.fit(x_train.values, y_train.values)\n",
    "y_hgbr_test = hgBoost.predict(x_test.values)\n",
    "\n",
    "#print(r2_score(y_test, y_hgbr_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gBoost_train = y_gBoost_train.reshape((y_gBoost_train.shape[0], 1))\n",
    "y_rf_train = y_rf_train.reshape((y_rf_train.shape[0], 1))\n",
    "y_xgb_train = y_xgb_train.reshape((y_rf_train.shape[0], 1))\n",
    "\n",
    "tmp_train = np.concatenate((y_gBoost_train, y_rf_train),axis=1)\n",
    "x_nntrain = np.concatenate((tmp_train, y_xgb_train), axis=1)\n",
    "\n",
    "\n",
    "y_gBoost_test = y_gBoost_test.reshape((y_gBoost_test.shape[0], 1))\n",
    "y_rf_test = y_rf_test.reshape((y_rf_test.shape[0], 1))\n",
    "y_xgb_test = y_xgb_test.reshape((y_rf_test.shape[0], 1))\n",
    "\n",
    "tmp_test = np.concatenate((y_gBoost_test, y_rf_test),axis=1)\n",
    "x_nntest = np.concatenate((tmp_test, y_xgb_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nnet = MLPRegressor(random_state=50)\n",
    "nnet.fit(x_nntrain, y_train.values)\n",
    "\n",
    "y_nnet_test = nnet.predict(x_nntest)\n",
    "#print(r2_score(y_test, y_nnet_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Average of multiple regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading different data$\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train =  pd.read_csv(\"x_train_dev3.csv\")\n",
    "x_test =  pd.read_csv(\"x_test_dev3.csv\")\n",
    "\n",
    "y_train = pd.read_csv(\"y_train.csv\")['y']\n",
    "\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=5)\n",
    "\n",
    "#When doing rendu NN we train on x_train y_train and get output from x_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "votingRegressor = VotingRegressor([('est', hgBoost), ('model_xgb', model_xgb), ('GBoost', gBoost)])\n",
    "\n",
    "votingRegressor.fit(x_train.values, y_train.values)\n",
    "y_test_predict = votingRegressor.predict(x_test.values)  # dernieres val\n",
    "#print(r2_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id     0.000000\n",
      "y     48.468132\n",
      "dtype: float64\n",
      "id    775.000000\n",
      "y      87.905548\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#y_test_df = pd.DataFrame(data=y_test_predict)\n",
    "\n",
    "y_test_df = pd.DataFrame(data=y_nnet_test)\n",
    "y_test_df.columns = [\"y\"]\n",
    "y_test_df[\"id\"] = y_test_df.index\n",
    "y_test_df = y_test_df[[\"id\", \"y\"]]\n",
    "\n",
    "print(y_test_df.min())\n",
    "print(y_test_df.max())\n",
    "\n",
    "y_test_df.to_csv(\"y_predict_sub8.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
