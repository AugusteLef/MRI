{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.model_selection import train_test_split as split_TT\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, SelectKBest\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_origin =  pd.read_csv(\"x_train_wo_outlier_KNN.csv\")\n",
    "y_train_origin = pd.read_csv(\"y_train_wo_outlier.csv\")\n",
    "\n",
    "x_test =  pd.read_csv(\"x_test.csv\", delimiter=\",\", index_col='id')\n",
    "\n",
    "#formatting\n",
    "train_data = pd.merge(left=x_train_origin, right=y_train_origin, how='inner').drop(columns=['id'])\n",
    "x_train = x_train_origin.drop(columns=['id'])\n",
    "y = y_train_origin['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1090, 832)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.fillna(x_train.median())\n",
    "x_test = x_test.fillna(x_test.median())\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier drop\n",
    "\n",
    "###### We use Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num lines before drop :  (1090, 60)\n",
      "Num lines before drop :  (1090,)\n",
      "Num lines after drop :  (1028, 60)\n",
      "Num lines after drop :  (1028,)\n"
     ]
    }
   ],
   "source": [
    "clf = IsolationForest(contamination='auto', random_state=323)\n",
    "clf.fit(x_train)\n",
    "yesOrNoOutlier = clf.predict(x_train.values)\n",
    "\n",
    "print(\"Num lines before drop : \", x_train.shape)\n",
    "print(\"Num lines before drop : \", y_train.shape)\n",
    "\n",
    "\n",
    "for num,line in enumerate(yesOrNoOutlier):\n",
    "    if not line == 1:\n",
    "        x_train = x_train.drop([num])\n",
    "        y_train = y_train.drop([num])\n",
    "\n",
    "print(\"Num lines after drop : \", x_train.shape)\n",
    "print(\"Num lines after drop : \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We define all feature selection functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scale(X):\n",
    "    scaler = QuantileTransformer()\n",
    "    return scaler.fit_transform(X)\n",
    "\n",
    "#Correlation\n",
    "def CorrSelector(xy_data,threshold):\n",
    "    corr = xy_data.corr()\n",
    "    top_features = corr.index[abs(corr['y']>threshold)]\n",
    "    \n",
    "    ##Displays best features correlation as a table\n",
    "    #plt.subplots(figsize=(12, 8))\n",
    "    top_corr = xy_data[top_features].corr()\n",
    "    sns.heatmap(top_corr, annot=True)\n",
    "    plt.show()\n",
    "    \n",
    "    #Displays best features as graphs\n",
    "    col = corr.index[abs(corr['y']>0.38)]\n",
    "    sns.set(style='ticks')\n",
    "    sns.pairplot(xy_data[col],kind='reg')\n",
    "    \n",
    "    return top_features, top_corr\n",
    "\n",
    "\n",
    "#Remove Low Variance\n",
    "def HighVarSelector(X_train, X_test):\n",
    "    threshold = VarianceThreshold(threshold=(.95 * (1 - .95)))\n",
    "    \n",
    "    filtered_X_train = threshold.fit_transform(X_train)\n",
    "    filtered_X_test = threshold.transform(X_test)\n",
    "    return pd.DataFrame(filtered_X_train), pd.DataFrame(filtered_X_test)\n",
    "\n",
    "\n",
    "#KBest Selector with mutual information regression (removes similar data)\n",
    "def MutualInfoSelector(X_train,X_test,y,num_features):\n",
    "    #TODO remove random_state\n",
    "    k_best = SelectKBest(score_func=lambda X,y:mutual_info_regression(X, y, random_state=50), k=num_features)\n",
    "    X_train_kbest = k_best.fit_transform(X_train,y)\n",
    "    X_test_kbest = k_best.transform(X_test)\n",
    "    return pd.DataFrame(X_train_kbest),pd.DataFrame(X_test_kbest)\n",
    "\n",
    "\n",
    "#GradientBoost\n",
    "def GBoostSelector(X_train,X_test,y):\n",
    "    #TODO check params, remove random_state\n",
    "    GBoost = GradientBoostingRegressor(max_depth=20, n_estimators=300, min_samples_leaf=15, min_samples_split=10, random_state =50)\n",
    "    GBoost.fit(X_train.values, y.values)\n",
    "    \n",
    "    #Top features selection, sorts them by importance\n",
    "    top_df = pd.DataFrame(data=GBoost.feature_importances_, columns=['value'])\n",
    "    top_df['feature'] = np.asarray(top_df.index)\n",
    "    top_df = top_df.sort_values(axis=0, by='value', ascending=False)\n",
    "    #TODO vary this param in order to find optimal cut\n",
    "    top_df = top_df[top_df['value'] > 0.006]\n",
    "\n",
    "    #Displays best features and their score\n",
    "    sns.barplot(x=top_df['feature'], y=top_df['value'])\n",
    "    \n",
    "    gboost_selector = SelectFromModel(GBoost, prefit=True)\n",
    "    X_train_bestfeatures = gboost_selector.transform(X_train)\n",
    "    X_test_bestfeatures = gboost_selector.transform(X_test)\n",
    "    \n",
    "    return pd.DataFrame(X_train_bestfeatures),pd.DataFrame(X_test_bestfeatures)\n",
    "\n",
    "#Recursive Feature Elimination\n",
    "def RFE_selector(X_train,X_test,y,num_features):\n",
    "    estimator = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=4, min_samples_leaf=15, min_samples_split=10,random_state=50)\n",
    "    rfe_selector = RFE(estimator=estimator, n_features_to_select=num_features, step=0.1, verbose=1)\n",
    "    x_train_rfe = rfe_selector.fit_transform(X_train, y)\n",
    "    x_test_rfe = rfe_selector.transform(X_test)\n",
    "    rfe_support = rfe_selector.get_support()\n",
    "    rfe_feature = x_train.loc[:,rfe_support].columns.tolist()\n",
    "    return rfe_support, rfe_feature, pd.DataFrame(x_train_rfe),pd.DataFrame(x_test_rfe)\n",
    "\n",
    "\n",
    "#LightGBM\n",
    "def LightGBM(X_train,X_test,y,num_features):\n",
    "    print(\"GBM\")\n",
    "    lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n",
    "            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "    lgb_selector = SelectFromModel(lgbc)\n",
    "    #lgb_selector = SelectFromModel(lgbc, max_features=num_features)\n",
    "    lgb_selector.fit(X_train, y)\n",
    "    lgb_support = lgb_selector.get_support()\n",
    "    lgb_feature = X_train.loc[:,lgb_support].columns.tolist()\n",
    "    \n",
    "    x_train_lgb = lgb_selector.transform(X_train)\n",
    "    x_test_lgb = lgb_selector.transform(X_test)\n",
    "    \n",
    "    return lgb_support, lgb_feature, pd.DataFrame(x_train_lgb),pd.DataFrame(x_test_lgb)\n",
    "\n",
    "#Chi squared test\n",
    "def Chi_selector(X,y,num_features):\n",
    "    print(\"Chi\")\n",
    "    #Normalize values as required for Chi2 test\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    chi_selector = SelectKBest(chi2, k=num_features)\n",
    "    chi_selector.fit(X_norm, y)\n",
    "    chi_support = chi_selector.get_support()\n",
    "    chi_feature = X.loc[:,chi_support].columns.tolist()\n",
    "    return chi_support, chi_feature\n",
    "\n",
    "\n",
    "\n",
    "#Lasso : SelectFromModel (L1 norm)\n",
    "def LassoModel(X,y,num_features):\n",
    "    print(\"Lasso\")\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l1\",solver='liblinear'), max_features=num_features)\n",
    "    embeded_lr_selector.fit(X_norm, y)\n",
    "    embeded_lr_support = embeded_lr_selector.get_support()\n",
    "    embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\n",
    "    return embeded_lr_support, embeded_lr_feature\n",
    "\n",
    "\n",
    "#Tree-based : SelectFromModel\n",
    "def TreebasedModel(X,y,num_features):\n",
    "    print(\"TBM\")\n",
    "    embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_features)\n",
    "    embeded_rf_selector.fit(X, y)\n",
    "    embeded_rf_support = embeded_rf_selector.get_support()\n",
    "    embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\n",
    "    return embeded_rf_support, embeded_rf_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection takes place here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before Feature Selection: (1090, 832) \n",
      "Test shape before Feature Selection: (776, 832) \n",
      "Train shape after HighVarSelector: (1090, 721) \n",
      "Test shape after HighVarSelector: (776, 721) \n",
      "Train shape after MutualInfoSelector: (1090, 600) \n",
      "Test shape after MutualInfoSelector: (776, 600) \n",
      "Fitting estimator with 600 features.\n",
      "Fitting estimator with 540 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 360 features.\n",
      "Fitting estimator with 300 features.\n",
      "Fitting estimator with 240 features.\n",
      "Fitting estimator with 180 features.\n",
      "Fitting estimator with 120 features.\n",
      "Train shape after RFESelector: (1090, 100) \n",
      "Test shape after RFESelector: (776, 100) \n"
     ]
    }
   ],
   "source": [
    "#Set display options\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "#Init variables (number of retained top features)\n",
    "num_features=60\n",
    "feature_name = x_train.columns.tolist()\n",
    "\n",
    "print (\"Shape before Feature Selection: {} \".format(x_train.shape))\n",
    "print (\"Test shape before Feature Selection: {} \".format(x_test.shape))\n",
    "\n",
    "### HighVar -> MutualInfo -> GBoost dev2\n",
    "### HighVar -> MutualInfo -> RFE dev3 \n",
    "### dev4 MutualInfo only\n",
    "### dev5 GBoost only\n",
    "### dev6 RFE only\n",
    "### dev7 scale -> mutualinfo -> LightGBM -> gboost -> rfe\n",
    "### dev8 scale -> var -> mutualinfo600 -> rfe 80\n",
    "### dev9 scale -> var -> mutual info700 -> rfe 250\n",
    "### dev10 highvar 0.8 -> mutualinfo -> RFE 100\n",
    "\n",
    "\n",
    "###Scaling\n",
    "#x_train = Scale(x_train)\n",
    "#x_test = Scale(x_test)\n",
    "#print (\"Train shape after Scaling: {} \".format(x_train.shape))\n",
    "#print (\"Test shape after Scaling: {} \".format(x_test.shape))\n",
    "\n",
    "###Correlation not used\n",
    "#top_feat, top_corr = CorrSelector(train_data,0.3)\n",
    "#print \"Shape after CorrSelector: {} \".format(x_train.shape)\n",
    "\n",
    "###HighVar\n",
    "x_train, x_test = HighVarSelector(x_train,x_test)\n",
    "print (\"Train shape after HighVarSelector: {} \".format(x_train.shape))\n",
    "print (\"Test shape after HighVarSelector: {} \".format(x_test.shape))\n",
    "\n",
    "###Mutual Info\n",
    "x_train, x_test = MutualInfoSelector(x_train, x_test, y,600)\n",
    "print (\"Train shape after MutualInfoSelector: {} \".format(x_train.shape))\n",
    "print (\"Test shape after MutualInfoSelector: {} \".format(x_test.shape))\n",
    "\n",
    "#LightGBM\n",
    "#lgbm_support,_,x_train,x_test = LightGBM(x_train,x_test,y,num_features) #num features unused\n",
    "#print (\"Train shape after LGBMSelector: {} \".format(x_train.shape))\n",
    "#print (\"Test shape after LGBMSelector: {} \".format(x_test.shape))\n",
    "\n",
    "###GBoost\n",
    "#x_train,x_test = GBoostSelector(x_train,x_test, y)\n",
    "#print (\"Train shape after GBoostSelector: {} \".format(x_train.shape))\n",
    "#print (\"Test shape after GBoostSelector: {} \".format(x_test.shape))\n",
    "\n",
    "\n",
    "###RFESelector\n",
    "rfe_support,_,x_train,x_test = RFE_selector(x_train,x_test,y,100)\n",
    "print (\"Train shape after RFESelector: {} \".format(x_train.shape))\n",
    "print (\"Test shape after RFESelector: {} \".format(x_test.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train.to_csv('x_train_dev10.csv', index=False)\n",
    "x_test.to_csv('x_test_dev10.csv', index=False)\n",
    "\n",
    "\n",
    "###CHI\n",
    "\n",
    "###RFE\n",
    "\n",
    "\n",
    "###Lasso\n",
    "\n",
    "\n",
    "###TreeBased\n",
    "\n",
    "\n",
    "###LightGBM\n",
    "\n",
    "\n",
    "\n",
    "#chi_support,_ = Chi_selector(X,y,num_features)\n",
    "#rfe_support,_ = RFE_selector(X,y,num_features)\n",
    "#embeded_lr_support,_ = LassoModel(X,y,num_features)\n",
    "#embeded_rf_support,_ = TreebasedModel(X,y,num_features)\n",
    "#embeded_lgb_support,_ = LightGBM(X,y,num_features)\n",
    "\n",
    "# put all selection together\n",
    "#feature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,'Random Forest':embeded_rf_support, 'LightGBM':embeded_lgb_support})\n",
    "# count the selected times for each feature\n",
    "#feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
    "# display the top 100\n",
    "#feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
    "#feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "#feature_selection_df.head(num_features)\n",
    "#Count the selected times for each feature and sorts them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1090, 60)\n",
      "(1090,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#loading different data\n",
    "x_train =  pd.read_csv(\"x_train_dev6.csv\")\n",
    "y_train = pd.read_csv(\"y_train_wo_outlier.csv\")['y']\n",
    "\n",
    "x_test = pd.read_csv(\"x_test_dev6.csv\")\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num lines before drop :  (1090, 60)\n",
      "Num lines before drop :  (1090,)\n",
      "Num lines after drop :  (1028, 60)\n",
      "Num lines after drop :  (1028,)\n"
     ]
    }
   ],
   "source": [
    "clf = IsolationForest(contamination='auto', random_state=323)\n",
    "clf.fit(x_train)\n",
    "yesOrNoOutlier = clf.predict(x_train.values)\n",
    "\n",
    "print(\"Num lines before drop : \", x_train.shape)\n",
    "print(\"Num lines before drop : \", y_train.shape)\n",
    "\n",
    "\n",
    "for num,line in enumerate(yesOrNoOutlier):\n",
    "    if not line == 1:\n",
    "        x_train = x_train.drop([num])\n",
    "        y_train = y_train.drop([num])\n",
    "\n",
    "print(\"Num lines after drop : \", x_train.shape)\n",
    "print(\"Num lines after drop : \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "\n",
    "def r2_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train.values)\n",
    "    r2= cross_val_score(model, x_train.values, y_train.values, scoring=\"r2\", cv = kf, )\n",
    "    return(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=5)\n",
    "\n",
    "#When doing rendu NN we train on x_train y_train and get output from x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out multiple regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:    2.0s\n"
     ]
    },
    {
     "ename": "WorkerInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 252, in __call__\n    return [func(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 252, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\", line 498, in fit\n    n_stages = self._fit_stages(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\", line 555, in _fit_stages\n    raw_predictions = self._fit_stage(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\", line 211, in _fit_stage\n    tree.fit(X, residual, sample_weight=sample_weight,\n  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1242, in fit\n    super().fit(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 375, in fit\n    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\nKeyboardInterrupt\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 600, in __call__\n    raise WorkerInterrupt() from e\njoblib.my_exceptions.WorkerInterrupt\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mWorkerInterrupt\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-6551edb602ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mGBoost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlogreg_cv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGBoost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlogreg_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tuned hpyerparameters :(best parameters) \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogreg_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWorkerInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###Determine best params\n",
    "grid={\n",
    "    \"n_estimators\":[100,300],\n",
    "    \"learning_rate\":[0.1],\n",
    "    \"max_depth\":[i for i in range(3, 8)],\n",
    "    \"max_features\":[0.1, 0.3, 0.5, 0.8]}\n",
    "GBoost = GradientBoostingRegressor(min_samples_leaf=15, min_samples_split=10, random_state =5)\n",
    "logreg_cv=GridSearchCV(GBoost,grid,cv=10, scoring='r2', verbose=2,n_jobs=6)\n",
    "logreg_cv.fit(x_train.values, y_train.values)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5700541242391217\n"
     ]
    }
   ],
   "source": [
    "#Params dev6 : 'learning_rate': 0.1, 'max_depth': 3, 'max_features': 0.1, 'n_estimators': 300}, accuracy : 0.5343144785026357\n",
    "#run 2 : \n",
    "#Params dev9 : 'learning_rate': 0.1, 'max_depth': 4, 'max_features': 0.1, 'n_estimators': 300}, accuracy : 0.5726071179087822\n",
    "#Params dev3 : 'learning_rate': 0.1, 'max_depth': 5, 'max_features': 0.3, 'n_estimators': 300 , accuracy : 0.6028442048688526\n",
    "#params dev10 :'learning_rate': 0.1, 'max_depth': 6, 'max_features': 0.5, 'n_estimators': 300} accuracy : 0.5754784812686855\n",
    "\n",
    "gBoost = GradientBoostingRegressor(n_estimators=300, learning_rate=0.1,\n",
    "                                   max_depth=4, max_features=0.1 ,\n",
    "                                   min_samples_leaf=15, min_samples_split=10,\n",
    "                                   random_state =50)\n",
    "\n",
    "gBoost.fit(x_train.values, y_train.values)\n",
    "y_gBoost_train = gBoost.predict(x_train.values)\n",
    "y_gBoost_test = gBoost.predict(x_test.values)  #pour le rendu on garde ca\n",
    "\n",
    "print(r2_score(y_test, y_gBoost_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=6)]: Done 320 out of 320 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'max_depth': 12, 'max_features': 0.3, 'n_estimators': 100}\n",
      "accuracy : 0.5903424228525121\n"
     ]
    }
   ],
   "source": [
    "###Determine best params\n",
    "grid={\n",
    "    \"n_estimators\":[100,300],\n",
    "    \"max_depth\":[i for i in range(11, 15)],\n",
    "    \"max_features\":[0.1,0.3,0.5,0.7]\n",
    "}\n",
    "rf = RandomForestRegressor(random_state=5, max_features='sqrt')\n",
    "rf_cv=GridSearchCV(rf,grid,cv=10, scoring='r2', verbose=2,n_jobs = 6)\n",
    "rf_cv.fit(x_train.values, y_train.values)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",rf_cv.best_params_)\n",
    "print(\"accuracy :\",rf_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5471111614636097\n"
     ]
    }
   ],
   "source": [
    "#Params ok \n",
    "#Params dev6 :{'max_depth': 13, 'max_features': 0.3, 'n_estimators': 300},0.5118924822902385\n",
    "#run2 : \n",
    "#Params dev9 :{'max_depth': 13, 'max_features': 0.3, 'n_estimators': 300}, 0.5266540952907695\n",
    "#Params dev3 : (best parameters)  {'max_depth': 13, 'max_features': 1.0, 'n_estimators': 300},accuracy : 0.5763962183900193\n",
    "#dev3run2 : (best parameters)  {'max_depth': 12, 'max_features': 0.6, 'n_estimators': 300} accuracy : 0.5764937773744069\n",
    "#Params dev10 :(best parameters)  {'max_depth': 13, 'max_features': 0.7, 'n_estimators': 300,0.5423908837250347\n",
    "\n",
    "random_forest = RandomForestRegressor(max_depth=12, max_features=0.3, n_estimators=100, random_state=5)\n",
    "random_forest.fit(x_train.values, y_train.values)\n",
    "f_i = random_forest.feature_importances_\n",
    "\n",
    "y_rf_train = random_forest.predict(x_train.values)\n",
    "y_rf_test = random_forest.predict(x_test.values)\n",
    "\n",
    "print(r2_score(y_test, y_rf_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 21 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=6)]: Done 210 out of 210 | elapsed:   46.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'learning_rate': 0.1, 'max_depth': 6}\n",
      "accuracy : 0.6058820857673277\n"
     ]
    }
   ],
   "source": [
    "###Determining best params\n",
    "grid={\"max_depth\":[i for i in range(5, 12)],\n",
    "      \"learning_rate\":[0.1,0.3,1]\n",
    "     }\n",
    "model_xgb = xgb.XGBRegressor(n_estimators=300, random_state =5, nthread = -1)\n",
    "model_xgb_cv=GridSearchCV(model_xgb, grid, cv=10, scoring='r2', verbose=2,n_jobs = 6)\n",
    "model_xgb_cv.fit(x_train, y_train.values)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",model_xgb_cv.best_params_)\n",
    "print(\"accuracy :\",model_xgb_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5684779590252672\n"
     ]
    }
   ],
   "source": [
    "#Params ok\n",
    "#Params dev6 :best parameters)  {'max_depth': 6},0.5360371775608017\n",
    "#Params dev9 : 'max_depth': 5} accuracy : 0.49125569249095324\n",
    "#Params dev3 : (best parameters)  {'max_depth': 6},accuracy : 0.6015225706221253\n",
    "#params dev10 : (best parameters)  {'learning_rate': 0.1, 'max_depth': 5} accuracy : 0.5596718320787533\n",
    "\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(learning_rate=0.1, max_depth=6, \n",
    "                              n_estimators=300,\n",
    "                             random_state =5, nthread = -1)\n",
    "model_xgb.fit(x_train.values, y_train.values)\n",
    "y_xgb_train = model_xgb.predict(x_train.values)\n",
    "y_xgb_test = model_xgb.predict(x_test.values)\n",
    "\n",
    "print(r2_score(y_test, y_xgb_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hist Gradient Boosting not used cause n < 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 16 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=8)]: Done 160 out of 160 | elapsed:   30.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'learning_rate': 0.1, 'max_depth': 5, 'max_iter': 100}\n",
      "accuracy : 0.6153539098768438\n"
     ]
    }
   ],
   "source": [
    "###Determining best params \n",
    "#grid={\"max_depth\":[i for i in range(14, 20)]}\n",
    "\n",
    "#grid = {\n",
    "# \"max_iter\": [100,300],\n",
    "# \"learning_rate\": [0.1],\n",
    "# \"max_depth\" : [3,4,5,6,7,8,9,10],\n",
    "# }\n",
    "\n",
    "#est = HistGradientBoostingRegressor(random_state=50)\n",
    "#logreg_cv=GridSearchCV(est,grid,cv=10, scoring='r2', verbose=2,n_jobs = 8)\n",
    "#logreg_cv.fit(x_train.values, y_train.values)\n",
    "\n",
    "#print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "#print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5768517933056773\n"
     ]
    }
   ],
   "source": [
    "#Params OK\n",
    "#Params dev6 : 'max_depth': 16} accuracy : 0.49125569249095324\n",
    "#run2: (best parameters)  {'learning_rate': 0.1, 'max_depth': 10, 'max_iter': 100} accuracy : 0.6130383995857118\n",
    "#run3:{'learning_rate': 0.1, 'max_depth': 7, 'max_iter': 100} accuracy : 0.6144288715016057\n",
    "#run4:learning_rate': 0.1, 'max_depth': 5, 'max_iter': 100} accuracy : 0.6153539098768438\n",
    "#Params dev9 : 'max_depth': 15} accuracy : 0.5562018718799511\n",
    "#Params dev3 : (best parameters)  {'max_depth': 14} accuracy : 0.5795827146160243\n",
    "#params dev10 :best parameters)  {'max_depth': 17} accuracy : 0.5541621105073512\n",
    "\n",
    "hgBoost = HistGradientBoostingRegressor(max_depth=5,learning_rate=0.1 ,random_state=50, max_iter = 100)\n",
    "\n",
    "hgBoost.fit(x_train.values, y_train.values)\n",
    "y_hgbr_train = hgBoost.predict(x_train.values)\n",
    "y_hgbr_test = hgBoost.predict(x_test.values)\n",
    "\n",
    "print(r2_score(y_test, y_hgbr_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 16 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:   31.0s\n",
      "[Parallel(n_jobs=8)]: Done 160 out of 160 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'learning_rate': 3, 'n_estimators': 2500}\n",
      "accuracy : 0.5843089821947789\n"
     ]
    }
   ],
   "source": [
    "###Determining best params \n",
    "#grid={\"max_depth\":[i for i in range(14, 20)]}\n",
    "\n",
    "grid = {\"learning_rate\": [3],\n",
    "       \"n_estimators\":[2500]}\n",
    "\n",
    "#est = AdaBoostRegressor(random_state=50)\n",
    "#logreg_cv=GridSearchCV(est,grid,cv=10, scoring='r2', verbose=2,n_jobs = 8)\n",
    "#logreg_cv.fit(x_train.values, y_train.values)\n",
    "\n",
    "#print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "#print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost score  0.573958174379655\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostRegressor(random_state=5, n_estimators=2500, learning_rate=3)\n",
    "ada.fit(x_train.values, y_train.values)\n",
    "y_ada_train = ada.predict(x_train.values)\n",
    "y_ada_test = ada.predict(x_test.values)\n",
    "print(\"Adaboost score \",r2_score(y_test, y_ada_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gBoost_train = y_gBoost_train.reshape((y_gBoost_train.shape[0], 1))\n",
    "#y_rf_train = y_rf_train.reshape((y_rf_train.shape[0], 1))\n",
    "y_xgb_train = y_xgb_train.reshape((y_rf_train.shape[0], 1))\n",
    "y_hgBoost_train = y_hgbr_train.reshape((y_hgbr_train.shape[0], 1))\n",
    "y_adaboost_train = y_ada_train.reshape((y_ada_train.shape[0], 1))\n",
    "\n",
    "#tmp_train = np.concatenate((y_gBoost_train, y_rf_train),axis=1)\n",
    "tmp_train = np.concatenate((y_hgBoost_train, y_gBoost_train),axis=1)\n",
    "#tmp_train = np.concatenate((y_hgBoost_train, tmp_train),axis=1)\n",
    "tmp_train = np.concatenate((y_adaboost_train, tmp_train),axis=1)\n",
    "x_nntrain = np.concatenate((tmp_train, y_xgb_train), axis=1)\n",
    "\n",
    "\n",
    "y_gBoost_test = y_gBoost_test.reshape((y_gBoost_test.shape[0], 1))\n",
    "#y_rf_test = y_rf_test.reshape((y_rf_test.shape[0], 1))\n",
    "y_xgb_test = y_xgb_test.reshape((y_rf_test.shape[0], 1))\n",
    "y_hgBoost_test = y_hgbr_test.reshape((y_hgbr_test.shape[0], 1))\n",
    "y_adaboost_test = y_ada_test.reshape((y_ada_test.shape[0], 1))\n",
    "\n",
    "\n",
    "\n",
    "#tmp_test = np.concatenate((y_gBoost_test, y_rf_test),axis=1)\n",
    "tmp_test = np.concatenate((y_hgBoost_test, y_gBoost_test),axis=1)\n",
    "#tmp_test = np.concatenate((y_hgBoost_test, tmp_test),axis=1)\n",
    "tmp_test = np.concatenate((y_adaboost_test, tmp_test),axis=1)\n",
    "\n",
    "x_nntest = np.concatenate((tmp_test, y_xgb_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#param_grid = [\n",
    "#        {\n",
    "#            'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "#            'solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "#            'hidden_layer_sizes': [\n",
    "#             (1,),(2,),(3,),(4,),(5,),(6,),(7,),(8,),(9,),(10,),(11,), (12,),(13,),(14,),(15,),(16,),(17,),(18,),(19,),(20,),(21,)\n",
    "#             ]\n",
    "#        }\n",
    "#       ]\n",
    "#clf = GridSearchCV(MLPClassifier(max_iter = 1000), param_grid, cv=10,\n",
    "#                           scoring='r2')\n",
    "#clf.fit(x_nntrain, y_train.values)\n",
    "\n",
    "\n",
    "#print(\"Best parameters set found on development set:\")\n",
    "#print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nnet = MLPRegressor(random_state=50,max_iter = 1000)\n",
    "nnet.fit(x_nntrain, y_train.values)\n",
    "\n",
    "y_nnet_test = nnet.predict(x_nntest)\n",
    "#print(\"NN score\",r2_score(y_test, y_nnet_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Average of multiple regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading different data$\n",
    "\n",
    "#from sklearn.metrics import r2_score\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#x_train =  pd.read_csv(\"x_train_dev3.csv\")\n",
    "#x_test =  pd.read_csv(\"x_test_dev3.csv\")\n",
    "\n",
    "#y_train = pd.read_csv(\"y_train.csv\")['y']\n",
    "\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=5)\n",
    "\n",
    "#When doing rendu NN we train on x_train y_train and get output from x_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.589974681100122\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "#votingRegressor = VotingRegressor([('hist',hgBoost),('AdaBoost',ada)])\n",
    "#votingRegressor = VotingRegressor([('hist',hgBoost), ('model_xgb', model_xgb),('GBoost',gBoost),('Adaboost',ada)])\n",
    "votingRegressor = VotingRegressor([('hist',hgBoost),('est', random_forest), ('model_xgb', model_xgb),('GBoost',gBoost),('Adaboost',ada)])\n",
    "\n",
    "\n",
    "\n",
    "votingRegressor.fit(x_train.values, y_train.values)\n",
    "y_test_predict = votingRegressor.predict(x_test.values)  # dernieres val\n",
    "print(r2_score(y_test, y_test_predict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id     0.000000\n",
      "y     49.555981\n",
      "dtype: float64\n",
      "id    775.000000\n",
      "y      85.736298\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y_test_df = pd.DataFrame(data=y_test_predict)\n",
    "\n",
    "#y_test_df = pd.DataFrame(data=y_nnet_test)\n",
    "y_test_df.columns = [\"y\"]\n",
    "y_test_df[\"id\"] = y_test_df.index\n",
    "y_test_df = y_test_df[[\"id\", \"y\"]]\n",
    "\n",
    "print(y_test_df.min())\n",
    "print(y_test_df.max())\n",
    "\n",
    "y_test_df.to_csv(\"y_predict_sub26.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.57 sub18\n",
    "#sub20 voting dev3 test 0.615 (nn 0.614)\n",
    "#sub20 best params\n",
    "#sub21 dev 3 best params voting reg 0.612845281037929\n",
    "#sub22 dev10 \n",
    "#sub23 dev6 + outlier nn\n",
    "#sub24 dev6 + outline votingregr\n",
    "#sub25 dev6 votingregr sans random forest, avec ada\n",
    "#sub26 tuned hyperparams, everything but random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8671166507391169\n",
      "0.8883234977122186\n",
      "0.8862075169479747\n",
      "0.9884511418426016\n",
      "0.9071230852896086\n"
     ]
    }
   ],
   "source": [
    "subx =  pd.read_csv(\"y_predict_sub11.csv\")['y']\n",
    "sub20 =  pd.read_csv(\"y_predict_sub20.csv\")['y']\n",
    "sub25 =  pd.read_csv(\"y_predict_sub25.csv\")['y']\n",
    "sub26 =  pd.read_csv(\"y_predict_sub26.csv\")['y']\n",
    "\n",
    "\n",
    "print(r2_score(subx, sub20))\n",
    "print(r2_score(subx, sub25))\n",
    "print(r2_score(subx, sub26))\n",
    "\n",
    "print(r2_score(sub25, sub26))\n",
    "print(r2_score(sub25, sub20))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sub2520\n",
    "sub\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
